# -*- coding: utf-8 -*-
"""nb7-0-inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mEz19jVFXZq8Qh5azmSFHC5bNGJymfpW
"""

import numpy as np
import pandas as pd 
import torch,os
from torchvision import datasets ,transforms
from torchvision.io import read_image
from torch.utils.data import Dataset,DataLoader
import matplotlib.pyplot as plt
import torch.nn as nn
from torchvision import models
from torch import optim
from torch.utils.data import random_split
from sklearn.model_selection import train_test_split

train_data=pd.read_csv('../input/petfinder-pawpularity-score/train.csv')
test_data=pd.read_csv('../input/petfinder-pawpularity-score/test.csv')

class ImageDataset(Dataset):    
    def __init__(self, img_dir, transform=None, target_transform=None):
        self.img_labels = train_data[['Id','Pawpularity']].copy()
        self.img_dir = img_dir 
        self.transform = transform
        self.target_transform = target_transform
    
    def __len__(self):
        return len(self.img_labels)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path+'.jpg')
        image_labels = self.img_labels.iloc[idx,1]
        if self.transform:
            image = self.transform(image)
        return image,image_labels

#dataset for testing
class ImageDataset_test(Dataset):
    
    def __init__(self, img_dir, transform=None, target_transform=None):
        self.img_id = test_data[['Id']].copy()
        self.img_dir = img_dir 
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_id)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_id.iloc[idx, 0])
        image = read_image(img_path+'.jpg')
        image_id = self.img_id.iloc[idx,0]
        if self.transform:
            image = self.transform(image)
        return image,image_id

transformed_dataset =  ImageDataset(img_dir='../input/petfinder-pawpularity-score/train/',transform=transforms.Compose([
    transforms.Resize(256),transforms.CenterCrop(224),transforms.RandomHorizontalFlip(),
     transforms.ConvertImageDtype(torch.float), transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])]))
val_size=int(0.2*len(transformed_dataset))
train_size=len(transformed_dataset)-val_size
train_dataset,val_dataset=random_split(transformed_dataset , (int(train_size),int(val_size)))
train_data_loader=DataLoader(train_dataset,batch_size=32,shuffle=True,num_workers=2)
val_data_loader=DataLoader(val_dataset,batch_size=32,shuffle=False,num_workers=2)

model=models.densenet161(pretrained=False)

for param in model.parameters(): #Freezing layers
    param.require_grad=False
model.classifier=nn.Sequential(nn.BatchNorm1d(2208),nn.ReLU(inplace=True),nn.Dropout(p=0.6),nn.Linear(2208,64),nn.BatchNorm1d(64),nn.ReLU(),nn.Linear(64,1))
for param in model.classifier.parameters():
    param.require_grad=True

model.load_state_dict(torch.load('../input/k/mohdrehan001/nb7-0-dnet/model_weights.pth'))
model.to('cuda:0')

criterion = nn.MSELoss()
optimizer = optim.Adam(model.classifier.parameters(),weight_decay=4e-4,lr=1e-3)
reduce_lr= optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.6,verbose=True)

num_epochs=0
running_loss=0.0
for epoch in range(num_epochs):
    model.train()
    running_loss=0.0
    for i,(train_features,train_labels) in enumerate(train_data_loader):   
        train_features=train_features.to('cuda:0')
        train_labels=train_labels.to('cuda:0')
        train_labels=train_labels.to(torch.float32)
        train_labels=train_labels.view(-1,1)
        out=model(train_features)
        loss = criterion(out, train_labels)
        with torch.no_grad():
            running_loss+=np.sqrt(loss.item()) 
        #print('epoch ',epoch+1,'/',num_epochs,'Loss:',np.sqrt(loss.item()),'iter:',i)
        # Backpropagation
        loss.backward()
        # Update model parameters
        optimizer.step()
        optimizer.zero_grad()
        del out,loss,train_features,train_labels
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    
    with torch.no_grad():
        print('avg loss after ',epoch+1,' epochs:',running_loss/248)
    #with torch.no_grad():
        val_loss_total=0
        model.eval()
        for i,(val_features,val_labels) in enumerate(val_data_loader):
            val_features,val_labels=next(iter(val_data_loader))
            val_features=val_features.to('cuda:0')
            val_labels=val_labels.to('cuda:0')
            val_out=model(val_features)
            val_labels=val_labels.view(-1,1)
            val_loss=criterion(val_out,val_labels)
            
        #print("validation loss:",np.sqrt(val_loss.item()))
            val_loss_total+=np.sqrt(val_loss.item())
            del val_out,val_loss,val_features,val_labels
            
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        reduce_lr.step()
        print('validation loss after epoch ',epoch+1,':',val_loss_total/62)
        #if((val_loss_total/31)<best_val):
            #best_val=val_loss_total/31
            #best_model=model

test_dataset =  ImageDataset_test(img_dir='../input/petfinder-pawpularity-score/test/',transform=transforms.Compose([
    transforms.Resize(256),transforms.CenterCrop(224), 
     transforms.ConvertImageDtype(torch.float), transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])]))
test_data_loader=DataLoader(test_dataset,batch_size=1,shuffle=False,num_workers=2)

test_out_data=[]
test_id_data=[]
for i,(test_img,test_id) in enumerate(test_data_loader):
    
    test_img=test_img.to('cuda:0')
    model.eval()
    with torch.no_grad():
        test_out=model(test_img)
        test_out=test_out.to('cpu')
    test_out_data.append(test_out)
    test_id_data.append(test_id)

#----------------------------------------------------------------------------------------------------    
test_out_data=np.array(test_out_data)
test_id_data=np.array(test_id_data)
test_id_data=np.reshape(test_id_data,test_id_data.shape[0])

submit=pd.DataFrame({'Id':test_id_data,'Pawpularity':test_out_data})
submit.set_index('Id')

submit.to_csv('submission.csv',index=False)

